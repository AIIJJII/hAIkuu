import nltk
import sys
import json

def whitesp_tokenizer(line):
	words = line.split()
	return words

def slash_tokenizer(haiku):
	lines = haiku.split('/')
	return lines


def main():
	
	with open('reddit_haikus.txt','r') as fin:
		data = fin.readlines()
	d = {}	
	cnt = 1
	for haiku in data:
		lines = slash_tokenizer(haiku)
		haikuPosList = []
		for line in lines:
			words = whitesp_tokenizer(line)
			tags = nltk.pos_tag(words)
			linePosList = []
			for pos in tags:
		 		linePosList.append(pos)
			haikuPosList.append(linePosList)
		d[cnt] = haikuPosList
		cnt += 1
	


	



	print("Done")

	with open('output.json','w') as fout:
		json.dump(d,fout)


main()